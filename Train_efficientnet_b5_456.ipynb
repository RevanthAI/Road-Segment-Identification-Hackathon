{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow-io as tfio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import gctf\n",
    "import os\n",
    "import random \n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0,EfficientNetB3,EfficientNetB5\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,GlobalAveragePooling2D,BatchNormalization, Activation\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.losses import BinaryCrossentropy \n",
    "from tensorflow.keras.metrics import AUC"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class config:\n",
    "    seed=42\n",
    "    batch_size=4\n",
    "    IMG_SIZE=456\n",
    "    IMG_SHAPE=(IMG_SIZE,IMG_SIZE,3)\n",
    "    dropout_rate=0.4\n",
    "    num_classes=1\n",
    "    AUTOTUNE=tf.data.experimental.AUTOTUNE\n",
    "    N_SPLITS=5\n",
    "    learning_rate=1e-5\n",
    "    epochs=15\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def seed_all(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONASHSEED']=str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS']='1'\n",
    "\n",
    "seed_all(config.seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_data=pd.read_csv('data/Train.csv')\n",
    "test_data=pd.read_csv('data/Test.csv')\n",
    "sub=pd.read_csv('data/SampleSubmission.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "training_data='/media/revanth/01D7A0158DB621C0/competitions/zindi/weekend_hackathon_road_segmentation/'\n",
    "train_data['file_path']=training_data+train_data['Image_ID']+'.jpeg'\n",
    "test_data['file_path']=training_data+test_data['Image_ID']+'.jpeg'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_data['Target'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    3771\n",
       "0    3229\n",
       "Name: Target, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def process_train_data(image_path,label):\n",
    "    image=tf.io.read_file(image_path)\n",
    "    image=tf.io.decode_jpeg(image,channels=3)\n",
    "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_crop = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "            \n",
    "    # Flips\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    if p_spatial > 0.75:\n",
    "        image = tf.image.transpose(image)\n",
    "        \n",
    "    # Rotates\n",
    "    if p_rotate > 0.75:\n",
    "        image = tf.image.rot90(image, k = 3) # rotate 270ยบ\n",
    "    elif p_rotate > 0.5:\n",
    "        image = tf.image.rot90(image, k = 2) # rotate 180ยบ\n",
    "    elif p_rotate > 0.25:\n",
    "        image = tf.image.rot90(image, k = 1) # rotate 90ยบ\n",
    "        \n",
    "    # Pixel-level transforms\n",
    "    if p_pixel_1 >= 0.4:\n",
    "        image = tf.image.random_saturation(image, lower = 0.7, upper = 1.3)\n",
    "    if p_pixel_2 >= 0.4:\n",
    "        image = tf.image.random_contrast(image, lower = 0.8, upper = 1.2)\n",
    "    if p_pixel_3 >= 0.4:\n",
    "        image = tf.image.random_brightness(image, max_delta = 0.1)\n",
    "        \n",
    "    # Crops\n",
    "    if p_crop > 0.7:\n",
    "        if p_crop > 0.9:\n",
    "            image = tf.image.central_crop(image, central_fraction = 0.7)\n",
    "        elif p_crop > 0.8:\n",
    "            image = tf.image.central_crop(image, central_fraction = 0.8)\n",
    "        else:\n",
    "            image = tf.image.central_crop(image, central_fraction = 0.9)\n",
    "    # elif p_crop > 0.4:\n",
    "    #     crop_size = tf.random.uniform([], int(config.IMG_SIZE * 0.8), config.IMG_SIZE, dtype = tf.int32)\n",
    "    #     image = tf.image.random_crop(image, size = [crop_size, crop_size, 3])\n",
    "\n",
    "    image=tf.image.resize(image,size=[config.IMG_SIZE,config.IMG_SIZE])\n",
    "    image=tf.cast(image,dtype='float32')/255.0\n",
    "    return image, label\n",
    "\n",
    "def process_valid_data(image_path,label):\n",
    "    image=tf.io.read_file(image_path)\n",
    "    image=tf.io.decode_jpeg(image,channels=3)\n",
    "    image=tf.image.resize(image,size=[config.IMG_SIZE,config.IMG_SIZE])\n",
    "    image=tf.cast(image,dtype='float32')/255.0\n",
    "    return image, label\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def getDatasetFromDataframe(train_files, train_labels, val_files, val_labels):\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
    "    train_ds = train_ds.shuffle(len(train_files))\n",
    "    train_ds = train_ds.map(process_train_data , num_parallel_calls=16)\n",
    "    train_ds = train_ds.batch(config.batch_size)\n",
    "    train_ds = train_ds.prefetch(config.AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels))\n",
    "    val_ds = val_ds.map(process_valid_data , num_parallel_calls=16)\n",
    "    val_ds = val_ds.batch(config.batch_size)\n",
    "    val_ds = val_ds.prefetch(config.AUTOTUNE)\n",
    "    \n",
    "    return train_ds , val_ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "skf=StratifiedKFold(n_splits=config.N_SPLITS,shuffle=True,random_state=config.seed)\n",
    "x=train_data['file_path']\n",
    "y=train_data['Target']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_model():\n",
    "    eff=EfficientNetB5(include_top=False,weights='imagenet',input_shape=config.IMG_SHAPE)\n",
    "    eff.trainable=True\n",
    "    out1=GlobalAveragePooling2D()(eff.output)\n",
    "    out2=Dropout(config.dropout_rate)(out1)\n",
    "    out3=Dense(1,activation='sigmoid')(out2)\n",
    "\n",
    "    model=Model(eff.input,out3)\n",
    "    opt=Adam(learning_rate=config.learning_rate)\n",
    "    opt.get_gradients = gctf.centralized_gradients_for_optimizer(opt)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=BinaryCrossentropy(),\n",
    "        metrics=AUC())\n",
    "\n",
    "    return model \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "for fold ,(train_idx,test_idx) in enumerate(skf.split(x,y)):\n",
    "    print('#'*50)\n",
    "    print(f'FOLD NUMBER : {fold}')\n",
    "    print('#'*50)\n",
    "\n",
    "    x_train,x_test=x.loc[train_idx],x.loc[test_idx]\n",
    "    y_train,y_test=y.loc[train_idx],y.loc[test_idx]\n",
    "    \n",
    "    train_ds,val_ds=getDatasetFromDataframe(x_train.values,y_train.values,x_test.values,y_test.values)\n",
    "  \n",
    "    tf.keras.backend.clear_session()\n",
    "    model=get_model()\n",
    "    weight_path_save = f\"data/models/bestb0_384_model_{str(fold)}_.hdf5\"\n",
    "# last_weight_path = 'last_model.hdf5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(weight_path_save, \n",
    "                                monitor= 'val_auc', \n",
    "                                verbose=1, \n",
    "                                save_best_only=True, \n",
    "                                mode= 'max', \n",
    "                                save_weights_only = False)\n",
    "\n",
    "\n",
    "    early = EarlyStopping(monitor= 'val_auc', \n",
    "                        mode= 'max', \n",
    "                        patience=5)\n",
    "\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "    callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "    model.fit(train_ds,validation_data=val_ds,callbacks=callbacks_list,epochs=config.epochs,verbose=1)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "##################################################\n",
      "FOLD NUMBER : 0\n",
      "##################################################\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-05 21:07:42.330080: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-05 21:07:42.330749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-05 21:07:42.370029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.370548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:2d:00.0 name: GeForce RTX 2070 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-09-05 21:07:42.370565: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-09-05 21:07:42.371872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-09-05 21:07:42.371898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-09-05 21:07:42.373327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-05 21:07:42.373496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-05 21:07:42.374714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-05 21:07:42.375239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-09-05 21:07:42.377531: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-09-05 21:07:42.377663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.378505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.378862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-09-05 21:07:42.379224: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-05 21:07:42.379792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.380142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:2d:00.0 name: GeForce RTX 2070 computeCapability: 7.5\n",
      "coreClock: 1.62GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-09-05 21:07:42.380155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-09-05 21:07:42.380171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-09-05 21:07:42.380178: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-09-05 21:07:42.380185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-05 21:07:42.380193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-05 21:07:42.380200: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-05 21:07:42.380207: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-09-05 21:07:42.380215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-09-05 21:07:42.380256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.380614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.380933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-09-05 21:07:42.380951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-09-05 21:07:42.766215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-09-05 21:07:42.766256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-09-05 21:07:42.766265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-09-05 21:07:42.766507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.766885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.767223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-05 21:07:42.767540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6835 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:2d:00.0, compute capability: 7.5)\n",
      "2021-09-05 21:07:42.767716: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-05 21:07:45.713667: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-05 21:07:45.752015: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 4199740000 Hz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-05 21:07:58.487517: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-09-05 21:07:58.644408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-09-05 21:07:59.846472: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.852489: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.878419: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.887849: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.894693: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.896572: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.901414: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.903818: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.945472: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2021-09-05 21:07:59.947867: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.42GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1400/1400 [==============================] - 680s 476ms/step - loss: 0.6053 - auc: 0.7235 - val_loss: 0.4174 - val_auc: 0.9122\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.91224, saving model to data/models/bestb0_384_model_0_.hdf5\n",
      "Epoch 2/15\n",
      "1400/1400 [==============================] - 668s 477ms/step - loss: 0.4516 - auc: 0.8685 - val_loss: 0.3511 - val_auc: 0.9358\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.91224 to 0.93581, saving model to data/models/bestb0_384_model_0_.hdf5\n",
      "Epoch 3/15\n",
      "1400/1400 [==============================] - 684s 488ms/step - loss: 0.4335 - auc: 0.8784 - val_loss: 0.3175 - val_auc: 0.9444\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.93581 to 0.94436, saving model to data/models/bestb0_384_model_0_.hdf5\n",
      "Epoch 4/15\n",
      "1400/1400 [==============================] - 690s 493ms/step - loss: 0.4019 - auc: 0.8981 - val_loss: 0.3127 - val_auc: 0.9484\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.94436 to 0.94842, saving model to data/models/bestb0_384_model_0_.hdf5\n",
      "Epoch 5/15\n",
      "1400/1400 [==============================] - 702s 501ms/step - loss: 0.3702 - auc: 0.9136 - val_loss: 0.2871 - val_auc: 0.9519\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.94842 to 0.95190, saving model to data/models/bestb0_384_model_0_.hdf5\n",
      "Epoch 6/15\n",
      "1400/1400 [==============================] - 770s 550ms/step - loss: 0.3518 - auc: 0.9213 - val_loss: 0.2835 - val_auc: 0.9539\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.95190 to 0.95388, saving model to data/models/bestb0_384_model_0_.hdf5\n",
      "Epoch 7/15\n",
      "1400/1400 [==============================] - 824s 589ms/step - loss: 0.3105 - auc: 0.9392 - val_loss: 0.3023 - val_auc: 0.9468\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.95388\n",
      "Epoch 8/15\n",
      "1400/1400 [==============================] - 677s 483ms/step - loss: 0.3131 - auc: 0.9389 - val_loss: 0.2763 - val_auc: 0.9547\n",
      "\n",
      "Epoch 00008: val_auc improved from 0.95388 to 0.95470, saving model to data/models/bestb0_384_model_0_.hdf5\n",
      "Epoch 9/15\n",
      "1400/1400 [==============================] - 682s 487ms/step - loss: 0.2918 - auc: 0.9465 - val_loss: 0.2786 - val_auc: 0.9533\n",
      "\n",
      "Epoch 00009: val_auc did not improve from 0.95470\n",
      "Epoch 10/15\n",
      "1400/1400 [==============================] - 658s 470ms/step - loss: 0.2829 - auc: 0.9496 - val_loss: 0.2688 - val_auc: 0.9584\n",
      "\n",
      "Epoch 00010: val_auc improved from 0.95470 to 0.95838, saving model to data/models/bestb0_384_model_0_.hdf5\n",
      "Epoch 11/15\n",
      "1400/1400 [==============================] - 670s 479ms/step - loss: 0.2653 - auc: 0.9559 - val_loss: 0.2750 - val_auc: 0.9563\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.95838\n",
      "Epoch 12/15\n",
      "1400/1400 [==============================] - 666s 476ms/step - loss: 0.2834 - auc: 0.9500 - val_loss: 0.2710 - val_auc: 0.9555\n",
      "\n",
      "Epoch 00012: val_auc did not improve from 0.95838\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 13/15\n",
      "1400/1400 [==============================] - 661s 472ms/step - loss: 0.2514 - auc: 0.9616 - val_loss: 0.2647 - val_auc: 0.9580\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.95838\n",
      "Epoch 14/15\n",
      "1400/1400 [==============================] - 659s 471ms/step - loss: 0.2415 - auc: 0.9630 - val_loss: 0.2666 - val_auc: 0.9578\n",
      "\n",
      "Epoch 00014: val_auc did not improve from 0.95838\n",
      "Epoch 15/15\n",
      "1400/1400 [==============================] - 656s 468ms/step - loss: 0.2475 - auc: 0.9628 - val_loss: 0.2653 - val_auc: 0.9578\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.95838\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "##################################################\n",
      "FOLD NUMBER : 1\n",
      "##################################################\n",
      "Epoch 1/15\n",
      "1400/1400 [==============================] - 675s 473ms/step - loss: 0.6141 - auc: 0.7178 - val_loss: 0.4082 - val_auc: 0.9174\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.91740, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "Epoch 2/15\n",
      "1400/1400 [==============================] - 657s 469ms/step - loss: 0.4773 - auc: 0.8495 - val_loss: 0.3431 - val_auc: 0.9332\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.91740 to 0.93315, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "Epoch 3/15\n",
      "1400/1400 [==============================] - 661s 472ms/step - loss: 0.4411 - auc: 0.8755 - val_loss: 0.3305 - val_auc: 0.9366\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.93315 to 0.93661, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "Epoch 4/15\n",
      "1400/1400 [==============================] - 668s 477ms/step - loss: 0.4064 - auc: 0.8946 - val_loss: 0.3126 - val_auc: 0.9430\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.93661 to 0.94296, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "Epoch 5/15\n",
      "1400/1400 [==============================] - 653s 466ms/step - loss: 0.3717 - auc: 0.9144 - val_loss: 0.2976 - val_auc: 0.9470\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.94296 to 0.94704, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "Epoch 6/15\n",
      "1400/1400 [==============================] - 660s 472ms/step - loss: 0.3435 - auc: 0.9277 - val_loss: 0.2870 - val_auc: 0.9501\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.94704 to 0.95013, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "Epoch 7/15\n",
      "1400/1400 [==============================] - 658s 470ms/step - loss: 0.3255 - auc: 0.9346 - val_loss: 0.2950 - val_auc: 0.9499\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.95013\n",
      "Epoch 8/15\n",
      "1400/1400 [==============================] - 667s 476ms/step - loss: 0.3110 - auc: 0.9396 - val_loss: 0.2909 - val_auc: 0.9508\n",
      "\n",
      "Epoch 00008: val_auc improved from 0.95013 to 0.95079, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 9/15\n",
      "1400/1400 [==============================] - 668s 477ms/step - loss: 0.2793 - auc: 0.9525 - val_loss: 0.2807 - val_auc: 0.9545\n",
      "\n",
      "Epoch 00009: val_auc improved from 0.95079 to 0.95452, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "Epoch 10/15\n",
      "1400/1400 [==============================] - 663s 474ms/step - loss: 0.2931 - auc: 0.9470 - val_loss: 0.2781 - val_auc: 0.9540\n",
      "\n",
      "Epoch 00010: val_auc did not improve from 0.95452\n",
      "Epoch 11/15\n",
      "1400/1400 [==============================] - 659s 471ms/step - loss: 0.2777 - auc: 0.9529 - val_loss: 0.2771 - val_auc: 0.9540\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.95452\n",
      "Epoch 12/15\n",
      "1400/1400 [==============================] - 658s 470ms/step - loss: 0.2884 - auc: 0.9471 - val_loss: 0.2790 - val_auc: 0.9547\n",
      "\n",
      "Epoch 00012: val_auc improved from 0.95452 to 0.95473, saving model to data/models/bestb0_384_model_1_.hdf5\n",
      "Epoch 13/15\n",
      "1400/1400 [==============================] - 665s 475ms/step - loss: 0.2764 - auc: 0.9535 - val_loss: 0.2821 - val_auc: 0.9547\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.95473\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 14/15\n",
      "1400/1400 [==============================] - 666s 475ms/step - loss: 0.2775 - auc: 0.9528 - val_loss: 0.2783 - val_auc: 0.9546\n",
      "\n",
      "Epoch 00014: val_auc did not improve from 0.95473\n",
      "Epoch 15/15\n",
      "1400/1400 [==============================] - 660s 471ms/step - loss: 0.2876 - auc: 0.9485 - val_loss: 0.2759 - val_auc: 0.9546\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.95473\n",
      "##################################################\n",
      "FOLD NUMBER : 2\n",
      "##################################################\n",
      "Epoch 1/15\n",
      "1400/1400 [==============================] - 667s 467ms/step - loss: 0.6159 - auc: 0.7075 - val_loss: 0.3942 - val_auc: 0.9223\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.92227, saving model to data/models/bestb0_384_model_2_.hdf5\n",
      "Epoch 2/15\n",
      "1400/1400 [==============================] - 655s 468ms/step - loss: 0.4591 - auc: 0.8632 - val_loss: 0.3385 - val_auc: 0.9350\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.92227 to 0.93503, saving model to data/models/bestb0_384_model_2_.hdf5\n",
      "Epoch 3/15\n",
      "1400/1400 [==============================] - 663s 473ms/step - loss: 0.4283 - auc: 0.8830 - val_loss: 0.3569 - val_auc: 0.9368\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.93503 to 0.93682, saving model to data/models/bestb0_384_model_2_.hdf5\n",
      "Epoch 4/15\n",
      "1400/1400 [==============================] - 655s 468ms/step - loss: 0.4002 - auc: 0.8989 - val_loss: 0.2991 - val_auc: 0.9487\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.93682 to 0.94874, saving model to data/models/bestb0_384_model_2_.hdf5\n",
      "Epoch 5/15\n",
      "1400/1400 [==============================] - 664s 474ms/step - loss: 0.3701 - auc: 0.9149 - val_loss: 0.2909 - val_auc: 0.9511\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.94874 to 0.95108, saving model to data/models/bestb0_384_model_2_.hdf5\n",
      "Epoch 6/15\n",
      "1400/1400 [==============================] - 653s 467ms/step - loss: 0.3576 - auc: 0.9201 - val_loss: 0.2868 - val_auc: 0.9516\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.95108 to 0.95165, saving model to data/models/bestb0_384_model_2_.hdf5\n",
      "Epoch 7/15\n",
      "1400/1400 [==============================] - 656s 469ms/step - loss: 0.3104 - auc: 0.9406 - val_loss: 0.2915 - val_auc: 0.9494\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.95165\n",
      "Epoch 8/15\n",
      "1400/1400 [==============================] - 651s 465ms/step - loss: 0.2936 - auc: 0.9472 - val_loss: 0.3033 - val_auc: 0.9443\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.95165\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 9/15\n",
      "1400/1400 [==============================] - 655s 468ms/step - loss: 0.2970 - auc: 0.9471 - val_loss: 0.2794 - val_auc: 0.9529\n",
      "\n",
      "Epoch 00009: val_auc improved from 0.95165 to 0.95292, saving model to data/models/bestb0_384_model_2_.hdf5\n",
      "Epoch 10/15\n",
      "  19/1400 [..............................] - ETA: 10:24 - loss: 0.3299 - auc: 0.9703"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22814/2576463617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mreduceLROnPlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduceLROnPlat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tf.keras.backend.clear_session()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('tf_gpu': conda)"
  },
  "interpreter": {
   "hash": "d33e21431fad353c8fdb94506a08045bc3896bf1ad0a6f0b2b9437cafb4d281f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}